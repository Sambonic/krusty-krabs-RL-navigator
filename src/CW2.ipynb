{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gym pygame tensorflow numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Miscellaneous\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 1\n",
    "ORIGINAL_SCREEN_WIDTH, ORIGINAL_SCREEN_HEIGHT = int(2616 / 2.2), int(1816 / 2.2)\n",
    "SCREEN_WIDTH, SCREEN_HEIGHT = int(ORIGINAL_SCREEN_WIDTH * SCALE_FACTOR), int(ORIGINAL_SCREEN_HEIGHT * SCALE_FACTOR)\n",
    "CELL_SIZE = max(1, int(5 * SCALE_FACTOR))\n",
    "GRID_WIDTH, GRID_HEIGHT = SCREEN_WIDTH // CELL_SIZE, SCREEN_HEIGHT // CELL_SIZE\n",
    "AGENT_SIZE = 3\n",
    "TARGET_SIZE = 5\n",
    "FPS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.998\n",
    "TARGET_UPDATE = 100\n",
    "MEMORY_SIZE = 100000\n",
    "LEARNING_RATE =  0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Paths Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd() , \"..\"))\n",
    "\n",
    "# Path to images\n",
    "ASSETS_DIR = os.path.join(ROOT_DIR, \"images\", \"proj_assets\")\n",
    "\n",
    "# Image paths\n",
    "IMAGE_PATHS = {\n",
    "    \"env\": os.path.join(ASSETS_DIR, \"env2.png\"),\n",
    "    \"agent\":os.path.join(ASSETS_DIR, \"crabs.png\"),\n",
    "    \"target\": os.path.join(ASSETS_DIR, \"moneybag.png\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RobotEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Right, Down, Left\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(12,), dtype=np.float32\n",
    "        )\n",
    "        self.env = self._create_occupancy_grid()\n",
    "        self._add_obstacles()\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.episode_outcomes = []\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.target_image = pygame.transform.scale(pygame.image.load(IMAGE_PATHS[\"target\"]), (TARGET_SIZE * CELL_SIZE, TARGET_SIZE * CELL_SIZE))\n",
    "        self.agent_image = pygame.transform.scale(pygame.image.load(IMAGE_PATHS[\"agent\"]), (AGENT_SIZE * CELL_SIZE, AGENT_SIZE * CELL_SIZE))\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def _create_occupancy_grid(self):\n",
    "        return np.zeros((GRID_WIDTH, GRID_HEIGHT))\n",
    "\n",
    "    def _add_circular_obstacle(self, center, radius):\n",
    "        cx, cy = int(center[0] * SCALE_FACTOR), int(center[1] * SCALE_FACTOR)\n",
    "        scaled_radius = int(radius * SCALE_FACTOR)\n",
    "        for x in range(GRID_WIDTH):\n",
    "            for y in range(GRID_HEIGHT):\n",
    "                if (x + 0.5 - cx) ** 2 + (y + 0.5 - cy) ** 2 <= scaled_radius ** 2:\n",
    "                    self.env[x][y] = 1\n",
    "\n",
    "    def _add_rectangular_obstacle(self, top_left, width, height):\n",
    "        start_x, start_y = int(top_left[0] * SCALE_FACTOR), int(top_left[1] * SCALE_FACTOR)\n",
    "        scaled_width, scaled_height = int(width * SCALE_FACTOR), int(height * SCALE_FACTOR)\n",
    "        for x in range(start_x, min(start_x + scaled_width, GRID_WIDTH)):\n",
    "            for y in range(start_y, min(start_y + scaled_height, GRID_HEIGHT)):\n",
    "                self.env[x][y] = 1\n",
    "\n",
    "    def _add_obstacles(self):\n",
    "        # Frame\n",
    "        self._add_rectangular_obstacle((0, 1), GRID_WIDTH, 15)\n",
    "        self._add_rectangular_obstacle((0, 152), GRID_WIDTH, 15)\n",
    "        self._add_rectangular_obstacle((0, 1), 17, GRID_HEIGHT)\n",
    "        self._add_rectangular_obstacle((222, 1), 17, GRID_HEIGHT)\n",
    "\n",
    "        # Dining room (Tables then Barrels)\n",
    "        self._add_circular_obstacle((146, 133), 10)\n",
    "        self._add_circular_obstacle((74, 133), 10)\n",
    "        self._add_circular_obstacle((76, 102), 9)\n",
    "        self._add_circular_obstacle((38, 118), 11)\n",
    "        self._add_circular_obstacle((187, 118), 12)\n",
    "        self._add_circular_obstacle((157, 91), 10)\n",
    "\n",
    "        self._add_circular_obstacle((90, 115), 3)\n",
    "        self._add_circular_obstacle((162, 112), 3)\n",
    "        self._add_circular_obstacle((139, 100), 3)\n",
    "        self._add_circular_obstacle((181, 96), 3)\n",
    "        self._add_circular_obstacle((210, 123), 3)\n",
    "        self._add_circular_obstacle((55, 122), 3)\n",
    "        self._add_circular_obstacle((24, 93), 3)\n",
    "        self._add_circular_obstacle((24, 137), 3)\n",
    "        self._add_circular_obstacle((81, 145), 3)\n",
    "\n",
    "        self._add_rectangular_obstacle((101, 70), 28, 18)\n",
    "        self._add_circular_obstacle((115, 90), 14)\n",
    "\n",
    "        # Inner walls\n",
    "        self._add_rectangular_obstacle((0, 68), 33, 5)\n",
    "        self._add_rectangular_obstacle((46, 68), 89, 5)\n",
    "        self._add_rectangular_obstacle((153, 68), 38, 5)\n",
    "        self._add_rectangular_obstacle((208, 68), 38, 5)\n",
    "\n",
    "        self._add_rectangular_obstacle((68, 1), 5, 42)\n",
    "        self._add_rectangular_obstacle((68, 61), 5, 10)\n",
    "        self._add_rectangular_obstacle((183, 1), 5, 70)\n",
    "\n",
    "        # Kitchen area\n",
    "        self._add_rectangular_obstacle((70, 1), 12, 42)\n",
    "        self._add_rectangular_obstacle((104, 1), 23, 26)\n",
    "        self._add_rectangular_obstacle((101, 57), 27, 18)\n",
    "        self._add_rectangular_obstacle((81, 57), 20, 14)\n",
    "        self._add_rectangular_obstacle((177, 36), 8, 20)\n",
    "\n",
    "        self._add_circular_obstacle((135, 21), 3)\n",
    "        self._add_circular_obstacle((143, 21), 3)\n",
    "\n",
    "        # Crabs room\n",
    "        self._add_rectangular_obstacle((35, 32), 19, 12)\n",
    "        self._add_rectangular_obstacle((38, 24), 12, 12)\n",
    "\n",
    "        self._add_circular_obstacle((33, 52), 3)\n",
    "        self._add_circular_obstacle((53, 53), 3)\n",
    "\n",
    "        # Bathroom\n",
    "        self._add_rectangular_obstacle((185, 21), 14, 30)\n",
    "        self._add_rectangular_obstacle((185, 55), 10, 8)\n",
    "    \n",
    "    def _initialize_pygame(self, title=\"Double DQN Agent Simulation\"):\n",
    "        pygame.init()\n",
    "        screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        pygame.display.set_caption(title)\n",
    "        return screen\n",
    "\n",
    "    def _load_and_scale_image(self):\n",
    "        image = pygame.image.load(IMAGE_PATHS[\"env\"])\n",
    "        return pygame.transform.scale(image, (SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "    def _draw_occupancy_grid(self):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            for y in range(GRID_HEIGHT):\n",
    "                if self.env[x, y] == 1:\n",
    "                    pygame.draw.rect(self.screen, (128, 128, 128),\n",
    "                                (x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "\n",
    "    def _draw_image(self, screen, image, grid_x, grid_y, cell_size):\n",
    "\n",
    "        # Calculate the center position of the grid cell\n",
    "        center_x = grid_x * cell_size + cell_size // 2\n",
    "        center_y = grid_y * cell_size + cell_size // 2\n",
    "        \n",
    "        # Calculate the top-left corner for the image to be centered\n",
    "        top_left_x = center_x - image.get_width() // 2\n",
    "        top_left_y = center_y - image.get_height() // 2\n",
    "        \n",
    "        # Draw the image on the screen\n",
    "        screen.blit(image, (top_left_x, top_left_y))\n",
    "\n",
    "    def _is_valid_position(self, x, y, size):\n",
    "        for dx in range(size):\n",
    "            for dy in range(size):\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if nx < 0 or nx >= GRID_WIDTH or ny < 0 or ny >= GRID_HEIGHT or self.env[nx, ny] == 1:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def _get_state(self):\n",
    "        observation = []\n",
    "        for dx, dy in [(0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1)]:\n",
    "            nx, ny = self.x + dx * AGENT_SIZE, self.y + dy * AGENT_SIZE\n",
    "            if self._is_valid_position(nx, ny, AGENT_SIZE):\n",
    "                observation.append(0.0)\n",
    "            else:\n",
    "                observation.append(1.0)\n",
    "        state = [self.x / GRID_WIDTH, self.y / GRID_HEIGHT,\n",
    "                self.target_x / GRID_WIDTH, self.target_y / GRID_HEIGHT] + observation\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    def get_episode_outcomes(self):\n",
    "        return self.episode_outcomes\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]\n",
    "        new_x, new_y = self.x + dx, self.y + dy\n",
    "        new_distance = math.sqrt((new_x - self.target_x) ** 2 + (new_y - self.target_y) ** 2)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        info = {'target_reached': False}\n",
    "       \n",
    "        if self._is_valid_position(new_x, new_y, AGENT_SIZE):\n",
    "            self.x, self.y = new_x, new_y\n",
    "            target_left = self.target_x - TARGET_SIZE / 2\n",
    "            target_right = self.target_x + TARGET_SIZE / 2\n",
    "            target_top = self.target_y - TARGET_SIZE / 2\n",
    "            target_bottom = self.target_y + TARGET_SIZE / 2\n",
    "           \n",
    "            # Reaching target\n",
    "            if target_left <= self.x <= target_right and target_top <= self.y <= target_bottom:\n",
    "                reward = 50.0\n",
    "                done = True\n",
    "                info['target_reached'] = True\n",
    "                self.episode_outcomes.append(1)\n",
    "                return self._get_state(), reward, done, info\n",
    "               \n",
    "            # Distance-based reward with smoother scaling\n",
    "            distance_delta = self.current_distance - new_distance\n",
    "            if distance_delta > 0:\n",
    "                reward = distance_delta\n",
    "            else:\n",
    "                reward = distance_delta - 0.2\n",
    "               \n",
    "            reward -= 0.01\n",
    "            self.current_distance = new_distance\n",
    "        else:\n",
    "            reward = -5.0\n",
    "            done = True\n",
    "            self.episode_outcomes.append(0)\n",
    "           \n",
    "        if self.steps >= 500:\n",
    "            done = True\n",
    "            if done and not info['target_reached']:\n",
    "                self.episode_outcomes.append(0)\n",
    "           \n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = 50\n",
    "        self.y = 20\n",
    "        self.target_x = 145\n",
    "        self.target_y = 115\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Calculate initial distance for reward scaling\n",
    "        self.initial_distance = math.sqrt((self.x - self.target_x) ** 2 + (self.y - self.target_y) ** 2)\n",
    "        self.current_distance = self.initial_distance\n",
    "\n",
    "        # Keep track of episodes\n",
    "        self.current_episode += 1\n",
    "        \n",
    "        return self._get_state()\n",
    "\n",
    "    def render(self, reward=None, episode=None, draw_obstacles=None):\n",
    "        if self.screen is None:\n",
    "\n",
    "            self.screen = self._initialize_pygame()\n",
    "            self.background_image = self._load_and_scale_image()\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Fill the screen with the background image\n",
    "        self.screen.blit(self.background_image, (0, 0))\n",
    "        \n",
    "        # Draw obstacles\n",
    "        if draw_obstacles is not None:\n",
    "            self._draw_occupancy_grid()\n",
    "        \n",
    "        # Draw the target image\n",
    "        self._draw_image(self.screen, self.target_image, self.target_x, self.target_y, CELL_SIZE)\n",
    "\n",
    "        # Draw the agent image\n",
    "        self._draw_image(self.screen, self.agent_image, self.x, self.y, TARGET_SIZE)\n",
    "        \n",
    "        # Render reward and episode number\n",
    "        if reward is not None and episode is not None:\n",
    "            font = pygame.font.Font(None, 36)\n",
    "            text = font.render(f'Episode: {episode}  Reward: {reward:.2f}', True, (255, 255, 255))\n",
    "            self.screen.blit(text, (450, 25))\n",
    "        \n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize experience replay memory\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.policy_net = self._build_network()\n",
    "        self.target_net = self._build_network()\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "        \n",
    "        # Training steps counter\n",
    "        self.steps_done = 0\n",
    "        self.train_step_counter = tf.Variable(0)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        \n",
    "    def _build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        if training:\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                          math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "            self.steps_done += 1\n",
    "            \n",
    "            if sample > eps_threshold:\n",
    "                state_tensor = tf.convert_to_tensor(state)\n",
    "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "                action_values = self.policy_net(state_tensor, training=False)\n",
    "                return tf.argmax(action_values[0]).numpy()\n",
    "            else:\n",
    "                return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_values = self.policy_net(state_tensor, training=False)\n",
    "            return tf.argmax(action_values[0]).numpy()\n",
    "    \n",
    "    @tf.function\n",
    "    def _train_step(self, states, actions, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Current Q values\n",
    "            current_q_values = self.policy_net(states, training=True)\n",
    "            current_q = tf.reduce_sum(current_q_values * tf.one_hot(actions, self.action_size), axis=1)\n",
    "            \n",
    "            # Double DQN Target Q values\n",
    "            next_q_values_policy = self.policy_net(next_states, training=False)\n",
    "            next_actions = tf.argmax(next_q_values_policy, axis=1)\n",
    "            next_q_values_target = self.target_net(next_states, training=False)\n",
    "            max_next_q = tf.reduce_sum(next_q_values_target * tf.one_hot(next_actions, self.action_size), axis=1)\n",
    "            \n",
    "            target_q = rewards + (1 - tf.cast(dones, tf.float32)) * GAMMA * max_next_q\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = tf.reduce_mean(tf.square(target_q - current_q))\n",
    "        \n",
    "        # Compute gradients and update weights\n",
    "        gradients = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "        # Clip gradients\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policy_net.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "        \n",
    "        # Perform training step\n",
    "        loss = self._train_step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        # Update target network if needed\n",
    "        self.train_step_counter.assign_add(1)\n",
    "        if self.train_step_counter % TARGET_UPDATE == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        return loss.numpy()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        policy_filename = f\"{filename}_policy.weights.h5\"\n",
    "        target_filename = f\"{filename}_target.weights.h5\"\n",
    "\n",
    "        # Save policy network weights\n",
    "        self.policy_net.save_weights(policy_filename)\n",
    "        \n",
    "        # Save target network weights\n",
    "        self.target_net.save_weights(target_filename)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        # Adjust paths to match saved weight filenames\n",
    "        policy_path = f\"{filename}_policy.weights.h5\"\n",
    "        if os.path.exists(policy_path):\n",
    "            self.policy_net.load_weights(policy_path)\n",
    "\n",
    "        target_path = f\"{filename}_target.weights.h5\"\n",
    "        if os.path.exists(target_path):\n",
    "            self.target_net.load_weights(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Reward per Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(episode_rewards):\n",
    "    # Plot the training progress (total reward per episode)\n",
    "    plt.plot(range(len(episode_rewards)), episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Progress - Total Reward per Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Successful Episode Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_outcomes(episode_outcomes):\n",
    "    # Plot the outcomes (1 for success, 0 for failure)\n",
    "    plt.plot(episode_outcomes, marker='o', linestyle='-', color='b', markersize=4)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Outcome (1=Success, 0=Failure)')\n",
    "    plt.title('Episode Outcomes: Success or Failure')\n",
    "    plt.yticks([0, 1], ['Failure', 'Success'])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes=1000, model_prefix='robot_model'):\n",
    "    env = RobotEnv()\n",
    "    agent = DoubleDQNAgent(\n",
    "        state_size=env.observation_space.shape[0],\n",
    "        action_size=env.action_space.n,\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    best_reward = float('-inf')\n",
    "    episode_times = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(agent.memory) >= BATCH_SIZE:\n",
    "                loss = agent.train()\n",
    "                episode_losses.append(loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            env.render(reward=total_reward, episode=episode)\n",
    "            \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    env.close()\n",
    "                    return\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        episode_times.append(elapsed_time)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Save best model\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            agent.save_model(f\"{model_prefix}_best\")\n",
    "        \n",
    "        # Episodic save\n",
    "        if episode % 100 == 0:\n",
    "            agent.save_model(f\"{model_prefix}_ep{episode}\")\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = sum(episode_rewards[-10:]) / 10\n",
    "\n",
    "        print(f'Episode {episode}: Total Reward = {total_reward:.2f}, Avg Reward (10 ep) = {avg_reward:.2f}, Elapsed Time = {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    episode_outcomes = env.get_episode_outcomes()\n",
    "    env.close()\n",
    "\n",
    "    return episode_rewards, episode_outcomes, episode_losses, episode_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(num_episodes=100, model_prefix='robot_model_best'):\n",
    "    env = RobotEnv()\n",
    "    agent = DoubleDQNAgent(\n",
    "        state_size=env.observation_space.shape[0],\n",
    "        action_size=env.action_space.n,\n",
    "    )\n",
    "    \n",
    "    # Load the trained model\n",
    "    agent.load_model(model_prefix)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_times = [] \n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action using the trained policy (no exploration)\n",
    "            action = agent.select_action(state, training=False)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            env.render(reward=total_reward, episode=episode)\n",
    "            \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    env.close()\n",
    "                    return episode_rewards, env.get_episode_outcomes(), episode_times\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        episode_times.append(elapsed_time)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Total Reward = {total_reward:.2f}, Elapsed Time = {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Gather and print test statistics\n",
    "    avg_reward = sum(episode_rewards) / num_episodes\n",
    "    max_reward = max(episode_rewards)\n",
    "    min_reward = min(episode_rewards)\n",
    "    \n",
    "    print(f\"Test Completed: Avg Reward = {avg_reward:.2f}, Max Reward = {max_reward:.2f}, Min Reward = {min_reward:.2f}\")\n",
    "    \n",
    "    # Collect episode outcomes or statistics\n",
    "    episode_outcomes = env.get_episode_outcomes()\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards, episode_outcomes, episode_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, episode_outcomes, _, _ = train(130)\n",
    "print(\"Training completed! Average reward:\", sum(episode_rewards) / len(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_outcomes(episode_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize env and then load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, episode_outcomes, _ = test_agent(10)\n",
    "print(\"Testing completed! Average reward:\", sum(episode_rewards) / len(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_outcomes(episode_outcomes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
